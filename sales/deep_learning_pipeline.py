# -*- coding: utf-8 -*-
"""Housing Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AtntE-wHttTwq9hTNnioCLwr5BM5Lxgg
"""

# # ⚠️ RUN FIRST, then restart runtime: Runtime > Restart runtime
# !pip install -U -q \
#     tensorflow==2.18.0 \
#     scikit-learn==1.4.2 \
#     numpy==2.0.0 \
#     pandas==2.2.2 \
#     seaborn==0.13.2

import os, sys, logging, json
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

module_path = "/content"          # change if needed
if module_path not in sys.path:
    sys.path.append(module_path)

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s ▶ %(message)s",
                    datefmt="%H:%M:%S")

from lstm_simple_preprocessing import MultiZipPreprocessor

DATA_PATH = "/content/Zip_zhvi_bdrmcnt_1_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv"  # update

prep = MultiZipPreprocessor(DATA_PATH, lookback=24)
out  = prep.run()

print("Train / Val / Test shapes:")
for k, (X, y) in out["splits"].items():
    print(f"{k:5}  X {X.shape}  y {y.shape}")

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Concatenate, LSTM, Dropout, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

lookback  = out["lookback"]
n_num     = out["n_numeric"]
n_zip     = len(out["zip_lookup"])

# inputs
num_in = Input((lookback, n_num), name="num_in")
zip_in = Input((lookback,), dtype="int32", name="zip_in")

zip_emb = Embedding(n_zip, 16)(zip_in)
x = Concatenate(axis=-1)([num_in, zip_emb])
x = LSTM(128, return_sequences=True)(x)
x = Dropout(0.2)(x)
x = LSTM(64)(x)
x = Dropout(0.2)(x)
out_pred = Dense(1)(x)

model = Model([num_in, zip_in], out_pred)
model.compile(Adam(1e-3), loss="mse", metrics=["mae"])
model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

def split_inputs(X):
    n_num = out["n_numeric"]
    return [X[:, :, :n_num],
            X[:, :, n_num:].astype("int32").squeeze(-1)]

X_train, y_train = out["splits"]["train"]
X_val,   y_val   = out["splits"]["val"]

cb = [
    EarlyStopping(patience=10, restore_best_weights=True, monitor="val_loss"),
    ReduceLROnPlateau(patience=5, factor=0.5, monitor="val_loss"),
    ModelCheckpoint("best_global_lstm.h5", save_best_only=True,
                    monitor="val_loss", verbose=0)
]

history = model.fit(
    split_inputs(X_train), y_train,
    validation_data=(split_inputs(X_val), y_val),
    epochs=20,
    batch_size=64,
    verbose=1,
    callbacks=cb
)

fig, ax = plt.subplots(1, 2, figsize=(12,4))
ax[0].plot(history.history['loss'], label='train')
ax[0].plot(history.history['val_loss'], label='val')
ax[0].set_title("MSE loss"); ax[0].legend()
ax[1].plot(history.history['mae'], label='train')
ax[1].plot(history.history['val_mae'], label='val')
ax[1].set_title("MAE"); ax[1].legend()
plt.tight_layout(); plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import numpy as np

X_test, y_test_scaled = out["splits"]["test"]
y_pred_scaled = model.predict(split_inputs(X_test)).ravel()

# inverse-transform
scaler_y = out["scaler_y"]
y_test = scaler_y.inverse_transform(y_test_scaled.reshape(-1,1)).ravel()
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()

rmse = mean_squared_error(y_test, y_pred, squared=False)
mae  = mean_absolute_error(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred) * 100
print(f"Test RMSE ${rmse:,.0f} | MAE ${mae:,.0f} | MAPE {mape:.2f}%")

X_test, y_test_scaled = out["splits"]["test"]

# ---- 1. unscale both y_true and y_pred ----
y_true = out["scaler_y"].inverse_transform(y_test_scaled.reshape(-1,1)).ravel()

# lag-1 price in **scaled** units
lag1_scaled = X_test[:, -1, 0]
lag1_real   = out["scaler_y"].inverse_transform(lag1_scaled.reshape(-1,1)).ravel()

mape_naive = mean_absolute_percentage_error(y_true, lag1_real)*100
print(f"Naïve one-step MAPE  = {mape_naive:.2f}%")

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=1.0)
lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]
plt.plot(lims, lims, 'r--')
plt.xlabel("Actual");
plt.ylabel("Predicted");
plt.title("Predicted vs Actual ($)")
plt.show()

import numpy as np
import pandas as pd

def forecast_next_12_months(zip_code: int,
                            prep: MultiZipPreprocessor,
                            out: dict,
                            model) -> list[float]:
    """
    Predict the next 12 monthly prices for a single ZIP code
    using the already-trained global LSTM.
    Returns a list of 12 real-dollar prices.
    """
    lookback = out["lookback"]
    num_cols = [
        "lag_1", "lag_2", "lag_3", "lag_12",
        "rolling_mean_6", "pct_change_1",
        "sin_month", "cos_month",
    ]
    df_zip = prep.long[prep.long.RegionName == zip_code].copy().reset_index(drop=True)
    if len(df_zip) < lookback:
        raise ValueError(f"ZIP {zip_code} has only {len(df_zip)} rows (need ≥{lookback}).")

    zid    = out["zip_lookup"][zip_code]
    n_num  = out["n_numeric"]
    preds  = []

    # start with the most recent 24-month window
    window_df = df_zip.iloc[-lookback:].copy()

    for _ in range(12):                       # roll forward 12 steps
        # 1️⃣  build scaled window tensor
        X_num = window_df[num_cols].values.astype(float)
        X_num = out["scaler_X"].transform(X_num)
        X_full = np.hstack([X_num, np.full((lookback, 1), zid)])
        X_full = X_full[np.newaxis, :, :]

        X_input = [
            X_full[:, :, :n_num],                           # numeric feats
            X_full[:, :, n_num:].astype("int32").squeeze(-1)  # zip_id seq
        ]

        # 2️⃣  predict next-month price & inverse-scale
        y_scaled = model.predict(X_input, verbose=0).ravel()
        y_real   = out["scaler_y"].inverse_transform(y_scaled.reshape(-1, 1)).ravel()[0]
        preds.append(float(y_real))

        # 3️⃣  append the prediction to the window
        next_row = window_df.iloc[-1].copy()
        next_row["date"]  = next_row["date"] + pd.DateOffset(months=1)
        next_row["price"] = y_real
        window_df = pd.concat([window_df.iloc[1:], pd.DataFrame([next_row])]).reset_index(drop=True)

        # 4️⃣  recompute lag & rolling features for the updated window
        window_df["lag_1"]  = window_df["price"].shift(1)
        window_df["lag_2"]  = window_df["price"].shift(2)
        window_df["lag_3"]  = window_df["price"].shift(3)
        window_df["lag_12"] = window_df["price"].shift(12)
        window_df["rolling_mean_6"] = window_df["price"].shift(1).rolling(6).mean()
        window_df["pct_change_1"]   = window_df["price"].pct_change().shift(1)
        m = window_df["date"].dt.month
        window_df["sin_month"] = np.sin(2 * np.pi * m / 12)
        window_df["cos_month"] = np.cos(2 * np.pi * m / 12)
        window_df = window_df.fillna(method="bfill")        # fills first few NaNs

    return preds

from tensorflow.keras.models import load_model

model = load_model("best_global_lstm.h5", compile=False)

zip_code = 1013     # make sure it’s an int and exists in zip_lookup
forecast = forecast_next_12_months(zip_code, prep, out, model)

for i, price in enumerate(forecast, 1):
    print(f"Month +{i:2}:  ${price:,.0f}")

